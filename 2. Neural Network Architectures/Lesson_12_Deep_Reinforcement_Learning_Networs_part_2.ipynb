{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Policy Gradient Methods:"
      ],
      "metadata": {
        "id": "AEuN847PvfOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Deterministic Policy Gradient (DDPG):\n",
        "* Combines deep learning with policy gradients for continuous action spaces.\n",
        "* Maintains a deterministic policy and learns a Q-function using neural networks."
      ],
      "metadata": {
        "id": "xbGz13V7vijX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_4GL1ewvbqb",
        "outputId": "57d8fabd-c381-4f5c-9f6f-8ef85d0fc94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Reward: -1482.75\n",
            "Episode: 2, Reward: -1483.88\n",
            "Episode: 3, Reward: -1574.26\n",
            "Episode: 4, Reward: -1175.47\n",
            "Episode: 5, Reward: -1622.30\n",
            "Episode: 6, Reward: -1678.79\n",
            "Episode: 7, Reward: -1502.47\n",
            "Episode: 8, Reward: -1522.43\n",
            "Episode: 9, Reward: -1445.88\n",
            "Episode: 10, Reward: -1595.49\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def create_actor_network(state_dim, action_dim):\n",
        "    inputs = tf.keras.layers.Input(shape=(state_dim,))\n",
        "    net = tf.keras.layers.Dense(400, activation='relu')(inputs)\n",
        "    net = tf.keras.layers.Dense(300, activation='relu')(net)\n",
        "    outputs = tf.keras.layers.Dense(action_dim, activation='tanh')(net)  # tanh activation for bounded actions\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_critic_network(state_dim, action_dim):\n",
        "    state_input = tf.keras.layers.Input(shape=(state_dim,))\n",
        "    action_input = tf.keras.layers.Input(shape=(action_dim,))\n",
        "\n",
        "    # State pathway\n",
        "    state_net = tf.keras.layers.Dense(400, activation='relu')(state_input)\n",
        "    state_net = tf.keras.layers.Dense(300, activation=None)(state_net)\n",
        "\n",
        "    # Action pathway\n",
        "    action_net = tf.keras.layers.Dense(300, activation=None)(action_input)\n",
        "\n",
        "    # Combine state and action pathways\n",
        "    net = tf.keras.layers.Add()([state_net, action_net])\n",
        "    net = tf.keras.layers.Activation('relu')(net)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, activation=None)(net)  # Q-value output\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "    return model\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, action_high):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_high = action_high\n",
        "\n",
        "        # Initialize actor and critic networks\n",
        "        self.actor_network = create_actor_network(state_dim, action_dim)\n",
        "        self.critic_network = create_critic_network(state_dim, action_dim)\n",
        "\n",
        "        # Target networks for stability\n",
        "        self.target_actor_network = create_actor_network(state_dim, action_dim)\n",
        "        self.target_critic_network = create_critic_network(state_dim, action_dim)\n",
        "        self.target_actor_network.set_weights(self.actor_network.get_weights())\n",
        "        self.target_critic_network.set_weights(self.critic_network.get_weights())\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.tau = 0.005  # Soft update parameter\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = np.reshape(state, [1, self.state_dim])\n",
        "        action = self.actor_network(state)\n",
        "        action = self.action_high * action.numpy()[0]\n",
        "        return action\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        actor_weights = self.actor_network.get_weights()\n",
        "        critic_weights = self.critic_network.get_weights()\n",
        "        target_actor_weights = self.target_actor_network.get_weights()\n",
        "        target_critic_weights = self.target_critic_network.get_weights()\n",
        "\n",
        "        for i in range(len(actor_weights)):\n",
        "            target_actor_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * target_actor_weights[i]\n",
        "\n",
        "        for i in range(len(critic_weights)):\n",
        "            target_critic_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * target_critic_weights[i]\n",
        "\n",
        "        self.target_actor_network.set_weights(target_actor_weights)\n",
        "        self.target_critic_network.set_weights(target_critic_weights)\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        state = np.reshape(state, [1, self.state_dim])  # Reshape state to (1, state_dim)\n",
        "        next_state = np.reshape(next_state, [1, self.state_dim])  # Reshape next_state to (1, state_dim)\n",
        "        action = np.reshape(action, [1, self.action_dim])  # Reshape action to (1, action_dim)\n",
        "\n",
        "        # Convert inputs to tensors\n",
        "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        next_states = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
        "        terminals = tf.convert_to_tensor(done, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor_network(next_states)\n",
        "            target_q_values = self.target_critic_network([next_states, target_actions])\n",
        "            target_values = rewards + (1. - terminals) * 0.99 * target_q_values\n",
        "\n",
        "            q_values = self.critic_network([states, actions])\n",
        "            critic_loss = tf.keras.losses.MSE(target_values, q_values)\n",
        "\n",
        "        critic_grads = tape.gradient(critic_loss, self.critic_network.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_pred = self.actor_network(states)\n",
        "            critic_value = self.critic_network([states, actions_pred])\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grads = tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
        "\n",
        "        # Update target networks\n",
        "        self.update_target_networks()\n",
        "\n",
        "env = gym.make('Pendulum-v1')  # Use Pendulum-v1 instead of Pendulum-v0\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_high = env.action_space.high[0]\n",
        "\n",
        "agent = DDPGAgent(state_dim, action_dim, action_high)\n",
        "\n",
        "num_episodes = 10\n",
        "batch_size = 64\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(200):  # Maximum of 200 steps per episode\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Reward: {episode_reward:.2f}\")\n",
        "\n",
        "env.close()\n"
      ]
    }
  ]
}