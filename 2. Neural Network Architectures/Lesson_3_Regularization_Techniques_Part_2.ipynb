{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Dm7nMQAr7X"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert numpy arrays to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define a simple feedforward neural network\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(20,)),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create an instance of the model\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "nmO_lN67BWF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Temporal Smoothing with Accuracy Tracking"
      ],
      "metadata": {
        "id": "teweYIwqClRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal smoothing applies a penalty to the model's predictions based on the smoothness of the prediction trajectory over time. It helps in reducing model sensitivity to noisy inputs."
      ],
      "metadata": {
        "id": "GaY95LP2ClsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define a custom loss function with temporal smoothing regularization\n",
        "def temporal_smoothing_loss(y_true, y_pred):\n",
        "    smoothness_penalty = tf.reduce_mean(tf.square(y_pred[:, 1:] - y_pred[:, :-1]))\n",
        "    return tf.losses.sparse_categorical_crossentropy(y_true, y_pred) + smoothness_penalty\n",
        "\n",
        "# Compile your model using the custom loss function and accuracy metric\n",
        "model.compile(optimizer='adam', loss=temporal_smoothing_loss, metrics=['accuracy'])\n",
        "\n",
        "# Define a function to calculate accuracy on a dataset\n",
        "def calculate_accuracy(model, dataset):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for x_batch, y_batch in dataset:\n",
        "        predictions = model.predict(x_batch)\n",
        "        y_true.extend(y_batch.numpy())\n",
        "        y_pred.extend(tf.argmax(predictions, axis=1).numpy())\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Train the model with accuracy tracking\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for x_batch, y_batch in train_dataset:\n",
        "        model.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = calculate_accuracy(model, train_dataset)\n",
        "    print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSoGPNaFBpwW",
        "outputId": "77eb05e0-e623-4089-9999-02389950d268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 371ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Epoch 1, Training Accuracy: 0.9125\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Epoch 2, Training Accuracy: 0.92\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Epoch 3, Training Accuracy: 0.93\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "Epoch 4, Training Accuracy: 0.935\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 5, Training Accuracy: 0.945\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Epoch 6, Training Accuracy: 0.94\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Epoch 7, Training Accuracy: 0.93875\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Epoch 8, Training Accuracy: 0.95125\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Epoch 9, Training Accuracy: 0.955\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Epoch 10, Training Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. DropConnect"
      ],
      "metadata": {
        "id": "zBkz8ROiCt2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DropConnect is an extension of dropout where instead of dropping units (neurons), a randomly selected subset of weights within a layer are set to zero during training."
      ],
      "metadata": {
        "id": "u77M6n6yCvtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define a simple feedforward neural network\n",
        "def create_model2():\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(20,)),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create an instance of the model\n",
        "model = create_model2()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define a custom layer with DropConnect\n",
        "class DropConnect(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate=0.5):\n",
        "        super(DropConnect, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            mask = tf.random.uniform(tf.shape(inputs)) > self.rate\n",
        "            masked_inputs = inputs * tf.cast(mask, tf.float32)\n",
        "            return masked_inputs\n",
        "        return inputs\n",
        "\n",
        "# Use DropConnect in the model\n",
        "model.add(DropConnect(rate=0.5))\n",
        "model.add( Dense(2, activation='softmax'))\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC2PRMFaCywf",
        "outputId": "dc199a90-b26e-4f14-a9b0-29d120ac8954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 1s 3ms/step - loss: 0.7772 - accuracy: 0.5537\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6682 - accuracy: 0.6162\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.6800\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5199 - accuracy: 0.7400\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5173 - accuracy: 0.7312\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.7875\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7688\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8163\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8400\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8438\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3425 - accuracy: 0.8562\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8600\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.3249 - accuracy: 0.8675\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.3192 - accuracy: 0.8750\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8900\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8800\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.8950\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.8950\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.9187\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2471 - accuracy: 0.8963\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2494 - accuracy: 0.8988\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2395 - accuracy: 0.9050\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2378 - accuracy: 0.8975\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.9038\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2305 - accuracy: 0.9087\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2205 - accuracy: 0.9212\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2086 - accuracy: 0.9350\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2159 - accuracy: 0.9150\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2370 - accuracy: 0.9175\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2350 - accuracy: 0.9150\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2237 - accuracy: 0.9175\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9287\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9312\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9425\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9237\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2101 - accuracy: 0.9225\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9250\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9312\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9312\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9300\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9362\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9237\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9325\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1479 - accuracy: 0.9425\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9425\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9438\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9287\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9413\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9475\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9312\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7845b4804a90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Stochastic Depth"
      ],
      "metadata": {
        "id": "esQBqIjbDb5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Depth randomly skips entire layers during training, effectively training shorter networks within the full architecture. This regularizes the model and encourages robustness."
      ],
      "metadata": {
        "id": "t_9dWBSgDe7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# Define a custom layer with stochastic layer skipping\n",
        "class StochasticDepth(Layer):\n",
        "    def __init__(self, skip_prob=0.5):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.skip_prob = skip_prob\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            if tf.random.uniform(shape=[]) < self.skip_prob:\n",
        "                return inputs  # Skip layer\n",
        "            else:\n",
        "                return inputs  # Return inputs if not skipped during training\n",
        "        else:\n",
        "            return inputs  # Return inputs if not in training mode\n",
        "\n",
        "# Create an instance of the model\n",
        "model = create_model2()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Use StochasticDepth in the model\n",
        "model.add(StochasticDepth(skip_prob=0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgVFoCFuDez2",
        "outputId": "300b223c-4700-4ca5-faa5-270acba4ee36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 2s 6ms/step - loss: 0.8956 - accuracy: 0.5750\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5952 - accuracy: 0.6913\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.4756 - accuracy: 0.7738\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.4365 - accuracy: 0.7962\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8338\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.3535 - accuracy: 0.8462\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8475\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2842 - accuracy: 0.8725\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.3004 - accuracy: 0.8637\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8650\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.2663 - accuracy: 0.8863\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2419 - accuracy: 0.8975\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.8963\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9112\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9013\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9062\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.9075\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9250\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9350\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9087\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9262\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9013\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9275\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1971 - accuracy: 0.9125\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.9250\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1767 - accuracy: 0.9287\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9262\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9300\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.9312\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9375\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9388\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9425\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 0.9337\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1580 - accuracy: 0.9413\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1287 - accuracy: 0.9525\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9337\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9287\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9413\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.9400\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1454 - accuracy: 0.9538\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1644 - accuracy: 0.9438\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9500\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9438\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.9500\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9388\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9475\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.9500\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9600\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9400\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7845aed83a30>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Orthogonal Regularization"
      ],
      "metadata": {
        "id": "NnNT_l7BHaWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orthogonal regularization enforces orthogonal constraints on weight matrices, encouraging feature diversity and reducing redundancy."
      ],
      "metadata": {
        "id": "nBMO9LDcI_mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Custom layer with orthogonal weight regularization\n",
        "class OrthogonalRegularizationLayer(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(OrthogonalRegularizationLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  initializer='random_normal',\n",
        "                                  trainable=True,\n",
        "                                  regularizer=self.orthogonal_regularizer)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w)\n",
        "\n",
        "    def orthogonal_regularizer(self, w):\n",
        "        w = tf.transpose(w)\n",
        "        identity = tf.eye(tf.shape(w)[0])\n",
        "        ortho_loss = tf.reduce_mean(tf.square(tf.matmul(w, w, transpose_b=True) - identity))\n",
        "        return ortho_loss\n",
        "\n",
        "# Build a simple model using the custom layer\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    OrthogonalRegularizationLayer(128),\n",
        "    layers.ReLU(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSEvF2OEHaxG",
        "outputId": "73185e80-9448-49e1-c0eb-effcd0da1d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 17s 8ms/step - loss: 0.2827 - accuracy: 0.9266 - val_loss: 0.1733 - val_accuracy: 0.9569\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1558 - accuracy: 0.9628 - val_loss: 0.1331 - val_accuracy: 0.9659\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1245 - accuracy: 0.9719 - val_loss: 0.1243 - val_accuracy: 0.9707\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1077 - accuracy: 0.9753 - val_loss: 0.1073 - val_accuracy: 0.9741\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0961 - accuracy: 0.9776 - val_loss: 0.1106 - val_accuracy: 0.9719\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7845ac189f90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Adversarial Training (FGSM)"
      ],
      "metadata": {
        "id": "M453lcyyJNx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial training involves augmenting the training data with adversarial examples generated to fool the model, which helps in improving the model's robustness against adversarial attacks."
      ],
      "metadata": {
        "id": "kCMe67j4JQR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, datasets\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Reduce training set size for faster demonstration\n",
        "train_images = train_images[:2000]\n",
        "train_labels = train_labels[:2000]\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Function to generate adversarial examples using FGSM\n",
        "def generate_adversarial_example(model, image, label, epsilon=0.2):\n",
        "    image = tf.convert_to_tensor(image)  # Convert to tensor if not already\n",
        "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    label = tf.convert_to_tensor([label])  # Convert label to tensor\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        prediction = model(image)\n",
        "        loss = losses.sparse_categorical_crossentropy(label, prediction)\n",
        "\n",
        "    gradient = tape.gradient(loss, image)\n",
        "    perturbation = tf.sign(gradient)\n",
        "    adversarial_image = image + epsilon * perturbation\n",
        "    adversarial_image = tf.clip_by_value(adversarial_image, 0, 1)  # Clip to valid image range\n",
        "\n",
        "    return adversarial_image[0]  # Remove batch dimension and return\n",
        "\n",
        "# Adversarial training using model.fit\n",
        "epochs = 5\n",
        "epsilon = 0.2  # FGSM perturbation magnitude\n",
        "\n",
        "# Prepare adversarial examples for training\n",
        "adversarial_images = []\n",
        "for image, label in zip(train_images, train_labels):\n",
        "    adv_image = generate_adversarial_example(model, image, label, epsilon)\n",
        "    adversarial_images.append(adv_image)\n",
        "\n",
        "adversarial_images = tf.convert_to_tensor(adversarial_images)\n",
        "\n",
        "# Train the model using adversarial examples\n",
        "model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Final evaluation\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Final test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_gokyEuJVHl",
        "outputId": "ff52bbd8-c226-4283-a01f-b01fbb603f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 2s 16ms/step - loss: 1.1240 - accuracy: 0.6880 - val_loss: 0.5804 - val_accuracy: 0.8441\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.4478 - accuracy: 0.8855 - val_loss: 0.4302 - val_accuracy: 0.8778\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.3162 - accuracy: 0.9155 - val_loss: 0.3829 - val_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 2s 25ms/step - loss: 0.2577 - accuracy: 0.9360 - val_loss: 0.3723 - val_accuracy: 0.8912\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.2155 - accuracy: 0.9450 - val_loss: 0.3420 - val_accuracy: 0.8989\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8989\n",
            "Final test accuracy: 0.8988999724388123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Dynamic Weight Averaging"
      ],
      "metadata": {
        "id": "v4ZEIv-JLGv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Weight Averaging maintains a running average of model parameters during training, allowing the model to generalize better by averaging over multiple checkpoints."
      ],
      "metadata": {
        "id": "BrVwLZaSLIKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, datasets\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Custom callback for dynamic weight averaging\n",
        "class DynamicWeightAveragingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, decay=0.99):\n",
        "        super(DynamicWeightAveragingCallback, self).__init__()\n",
        "        self.decay = decay\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # Initialize averaged weights to the current model weights\n",
        "        self.averaged_weights = [tf.Variable(w) for w in self.model.get_weights()]\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        # Update averaged weights after each batch\n",
        "        current_weights = self.model.get_weights()\n",
        "        for avg_weight, weight in zip(self.averaged_weights, current_weights):\n",
        "            avg_weight.assign(self.decay * avg_weight + (1 - self.decay) * weight)\n",
        "\n",
        "# Train the model with dynamic weight averaging\n",
        "epochs = 5\n",
        "\n",
        "# Create an instance of the custom callback\n",
        "dynamic_weight_callback = DynamicWeightAveragingCallback()\n",
        "\n",
        "# Fit the model with the callback\n",
        "model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels),\n",
        "          callbacks=[dynamic_weight_callback])\n",
        "\n",
        "# Use averaged weights for inference\n",
        "model.set_weights([avg.numpy() for avg in dynamic_weight_callback.averaged_weights])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy (with dynamic weight averaging): {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMOCNBubLZkV",
        "outputId": "3d2c4404-bf05-4301-f3ec-d15dd38f3af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "   4/1875 [..............................] - ETA: 44s - loss: 2.2995 - accuracy: 0.1719  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0106s vs `on_train_batch_end` time: 0.0131s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 19s 9ms/step - loss: 0.2560 - accuracy: 0.9261 - val_loss: 0.1365 - val_accuracy: 0.9613\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1106 - accuracy: 0.9672 - val_loss: 0.0931 - val_accuracy: 0.9704\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0777 - accuracy: 0.9766 - val_loss: 0.0842 - val_accuracy: 0.9754\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0579 - accuracy: 0.9819 - val_loss: 0.0808 - val_accuracy: 0.9754\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0449 - accuracy: 0.9865 - val_loss: 0.0887 - val_accuracy: 0.9731\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0692 - accuracy: 0.9799\n",
            "Test accuracy (with dynamic weight averaging): 0.9799000024795532\n"
          ]
        }
      ]
    }
  ]
}