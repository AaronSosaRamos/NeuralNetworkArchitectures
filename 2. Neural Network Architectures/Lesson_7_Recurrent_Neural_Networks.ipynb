{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEW4UKHFqEOd"
      },
      "source": [
        "#Recurrent Neural Networks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n6wQ4Q8qhEX"
      },
      "source": [
        "A Recurrent Neural Network (RNN) is a type of artificial neural network designed to work with sequential data, where the order of inputs is crucial. Unlike traditional feedforward neural networks, RNNs have connections that form a directed cycle, allowing them to exhibit dynamic temporal behavior. This cyclic structure enables RNNs to maintain a memory of previous inputs, making them well-suited for tasks involving sequences like time series prediction, natural language processing (e.g., language modeling, machine translation), and speech recognition. RNNs process input sequences step-by-step, updating their internal state with each new input, which in turn influences subsequent outputs. However, standard RNNs suffer from issues like vanishing or exploding gradients over long sequences. To address this, variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) were developed, featuring specialized gating mechanisms that better control the flow of information and alleviate these problems. RNNs are implemented in frameworks like TensorFlow and PyTorch, offering powerful tools for training and deploying models that excel in handling sequential data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeAsru7gqDQI",
        "outputId": "219c303b-1c11-4526-b9c5-81651edb3efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 32)           320000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322113 (1.23 MB)\n",
            "Trainable params: 322113 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 8s 40ms/step - loss: 0.5989 - accuracy: 0.6580 - val_loss: 0.4505 - val_accuracy: 0.7964\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 7s 42ms/step - loss: 0.3433 - accuracy: 0.8598 - val_loss: 0.3674 - val_accuracy: 0.8436\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 6s 38ms/step - loss: 0.3067 - accuracy: 0.8734 - val_loss: 0.4136 - val_accuracy: 0.8238\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 7s 44ms/step - loss: 0.1840 - accuracy: 0.9317 - val_loss: 0.3912 - val_accuracy: 0.8356\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 7s 42ms/step - loss: 0.1043 - accuracy: 0.9675 - val_loss: 0.4320 - val_accuracy: 0.8436\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4493 - accuracy: 0.8408\n",
            "Test Accuracy: 0.8408\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load IMDb dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Preprocess data\n",
        "maxlen = 200  # Maximum sequence length\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "# Build RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=10000, output_dim=32, input_length=maxlen),\n",
        "    tf.keras.layers.SimpleRNN(units=32),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(test_data, test_labels)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIzjy6ptUcH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, GRU, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization, Dense\n",
        "\n",
        "# Define input shape and vocabulary size\n",
        "input_shape = (100,)  # Example input shape (sequence length)\n",
        "vocab_size = 10000\n",
        "\n",
        "# Input layer for variable-length sequences (e.g., padded text sequences)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Embedding layer to convert integer-encoded tokens to dense vectors\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=32)(input_layer)\n",
        "\n",
        "# RNN layers\n",
        "# SimpleRNN layer for basic sequence processing\n",
        "rnn_layer = SimpleRNN(units=64, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# GRU layer for more complex memory management than SimpleRNN\n",
        "gru_layer = GRU(units=64, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# LSTM layer for handling long-term dependencies in sequences\n",
        "lstm_layer = LSTM(units=64, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "# Processes input sequences in both forward and backward directions\n",
        "bidirectional_lstm = Bidirectional(LSTM(units=64, return_sequences=True))(embedding_layer)\n",
        "\n",
        "# Convolutional layer followed by Global Max Pooling\n",
        "# Conv1D layer with 64 filters and kernel size 3 for learning local patterns\n",
        "conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding_layer)\n",
        "# GlobalMaxPooling1D layer to reduce sequence length by selecting max value across each feature\n",
        "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
        "\n",
        "# Dropout and BatchNormalization layers\n",
        "# Dropout layer to randomly set input units to zero to prevent overfitting\n",
        "dropout_layer = Dropout(rate=0.5)(pooling_layer)\n",
        "# BatchNormalization layer to normalize activations, improving training stability\n",
        "batchnorm_layer = BatchNormalization()(dropout_layer)\n",
        "\n",
        "# Custom Attention layer (Bahdanau style)\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # Bahdanau attention mechanism\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector\n",
        "\n",
        "# Apply Attention mechanism to the LSTM output\n",
        "attention_layer = BahdanauAttention(units=64)(lstm_layer)\n",
        "\n",
        "# Dense output layer for classification\n",
        "output_layer = Dense(units=4, activation='softmax')(attention_layer)\n",
        "\n",
        "# Create a model with input and output layers\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary with detailed layer information\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydjL7aW8zTNA",
        "outputId": "fccd80ac-f5ed-4549-d771-9411f55dd650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3000/3000 [==============================] - 376s 125ms/step - loss: 1.3926 - accuracy: 0.2565 - val_loss: 1.3835 - val_accuracy: 0.2799\n",
            "Epoch 2/10\n",
            "3000/3000 [==============================] - 386s 129ms/step - loss: 1.3849 - accuracy: 0.2794 - val_loss: 1.3892 - val_accuracy: 0.2718\n",
            "Epoch 3/10\n",
            "3000/3000 [==============================] - 418s 139ms/step - loss: 1.3857 - accuracy: 0.2803 - val_loss: 1.3888 - val_accuracy: 0.2750\n",
            "Epoch 4/10\n",
            "3000/3000 [==============================] - 399s 133ms/step - loss: 1.3845 - accuracy: 0.2823 - val_loss: 1.3808 - val_accuracy: 0.2838\n",
            "Epoch 5/10\n",
            "3000/3000 [==============================] - 402s 134ms/step - loss: 1.3839 - accuracy: 0.2837 - val_loss: 1.3852 - val_accuracy: 0.2843\n",
            "Epoch 6/10\n",
            "3000/3000 [==============================] - 387s 129ms/step - loss: 1.3833 - accuracy: 0.2830 - val_loss: 1.3788 - val_accuracy: 0.2970\n",
            "Epoch 7/10\n",
            "3000/3000 [==============================] - 409s 137ms/step - loss: 1.3819 - accuracy: 0.2823 - val_loss: 1.3767 - val_accuracy: 0.2906\n",
            "Epoch 8/10\n",
            "3000/3000 [==============================] - 380s 127ms/step - loss: 1.3818 - accuracy: 0.2835 - val_loss: 1.3816 - val_accuracy: 0.2939\n",
            "Epoch 9/10\n",
            "3000/3000 [==============================] - 322s 107ms/step - loss: 1.3825 - accuracy: 0.2814 - val_loss: 1.3786 - val_accuracy: 0.2895\n",
            "Epoch 10/10\n",
            "3000/3000 [==============================] - 315s 105ms/step - loss: 1.3814 - accuracy: 0.2829 - val_loss: 1.3818 - val_accuracy: 0.2795\n",
            "238/238 [==============================] - 3s 12ms/step - loss: 1.3766 - accuracy: 0.2883\n",
            "Test Accuracy: 28.83%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load AG News dataset\n",
        "dataset, info = tfds.load('ag_news_subset', split='train', with_info=True)\n",
        "\n",
        "# Extract text and labels from the dataset\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for example in dataset:\n",
        "    texts.append(example['description'].numpy().decode('utf-8'))\n",
        "    labels.append(example['label'].numpy())\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "maxlen = 200  # choose a maximum sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Define the RNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen),\n",
        "    tf.keras.layers.SimpleRNN(64),  # Simple RNN layer\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # Output layer with 4 units (one for each class)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # monitor validation loss\n",
        "    patience=3,           # number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True  # restore model weights to the best iteration\n",
        ")\n",
        "\n",
        "# Train the model with EarlyStopping callback\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping_callback])  # include EarlyStopping callback\n",
        "\n",
        "# Load test data\n",
        "test_dataset = tfds.load('ag_news_subset', split='test')\n",
        "\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for example in test_dataset:\n",
        "    test_texts.append(example['description'].numpy().decode('utf-8'))\n",
        "    test_labels.append(example['label'].numpy())\n",
        "\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Tokenize and pad test data\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(padded_test_sequences, test_labels)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}